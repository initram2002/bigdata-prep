{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCount: RDD vs DataFrame Comparison\n",
    "\n",
    "Comparing RDD and DataFrame approaches for the classic WordCount problem in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount RDD vs DataFrame\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Sample text data\n",
    "sample_text = [\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing\",\n",
    "    \"Spark provides built-in modules for streaming SQL machine learning and graph processing\",\n",
    "    \"You can use Spark interactively from the Scala Python R and SQL shells\",\n",
    "    \"Spark runs on Hadoop Kubernetes Apache Mesos or standalone\",\n",
    "    \"It can access diverse data sources including HDFS Alluxio Apache Cassandra\",\n",
    "    \"Apache Spark achieves high performance for both batch and streaming data\",\n",
    "    \"Spark uses advanced DAG execution engine that supports acyclic data flow\",\n",
    "    \"The engine optimizes arbitrary operator graphs and supports in-memory computing\"\n",
    "]\n",
    "\n",
    "print(f\"Sample data: {len(sample_text)} lines\")\n",
    "for i, line in enumerate(sample_text[:3]):\n",
    "    print(f\"{i+1}: {line}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: RDD Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_rdd(text_data):\n",
    "    \"\"\"WordCount using RDD transformations\"\"\"\n",
    "    \n",
    "    # Create RDD from text data\n",
    "    lines_rdd = spark.sparkContext.parallelize(text_data)\n",
    "    \n",
    "    # Transform and count words\n",
    "    word_counts = lines_rdd \\\n",
    "        .flatMap(lambda line: re.findall(r'\\b\\w+\\b', line.lower())) \\\n",
    "        .filter(lambda word: len(word) > 2) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Execute RDD WordCount\n",
    "start_time = time.time()\n",
    "rdd_result = wordcount_rdd(sample_text)\n",
    "rdd_time = time.time() - start_time\n",
    "\n",
    "print(\"RDD WordCount Results (Top 10):\")\n",
    "for word, count in rdd_result.take(10):\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "print(f\"\\nRDD execution time: {rdd_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: DataFrame Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_dataframe(text_data):\n",
    "    \"\"\"WordCount using DataFrame operations\"\"\"\n",
    "    \n",
    "    # Create DataFrame from text data\n",
    "    df = spark.createDataFrame([(line,) for line in text_data], [\"line\"])\n",
    "    \n",
    "    # Transform and count words using DataFrame operations\n",
    "    word_counts = df \\\n",
    "        .select(explode(split(lower(col(\"line\")), \"\\\\W+\")).alias(\"word\")) \\\n",
    "        .filter(length(col(\"word\")) > 2) \\\n",
    "        .filter(col(\"word\") != \"\") \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .count() \\\n",
    "        .orderBy(desc(\"count\"))\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Execute DataFrame WordCount\n",
    "start_time = time.time()\n",
    "df_result = wordcount_dataframe(sample_text)\n",
    "df_time = time.time() - start_time\n",
    "\n",
    "print(\"DataFrame WordCount Results (Top 10):\")\n",
    "df_result.show(10)\n",
    "\n",
    "print(f\"DataFrame execution time: {df_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: SQL Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_sql(text_data):\n",
    "    \"\"\"WordCount using Spark SQL\"\"\"\n",
    "    \n",
    "    # Create DataFrame and register as temp view\n",
    "    df = spark.createDataFrame([(line,) for line in text_data], [\"line\"])\n",
    "    df.createOrReplaceTempView(\"text_data\")\n",
    "    \n",
    "    # Use SQL for word counting\n",
    "    word_counts = spark.sql(\"\"\"\n",
    "        SELECT word, COUNT(*) as count\n",
    "        FROM (\n",
    "            SELECT EXPLODE(SPLIT(LOWER(line), '\\\\W+')) as word\n",
    "            FROM text_data\n",
    "        ) words\n",
    "        WHERE LENGTH(word) > 2 AND word != ''\n",
    "        GROUP BY word\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "# Execute SQL WordCount\n",
    "start_time = time.time()\n",
    "sql_result = wordcount_sql(sample_text)\n",
    "sql_time = time.time() - start_time\n",
    "\n",
    "print(\"SQL WordCount Results (Top 10):\")\n",
    "sql_result.show(10)\n",
    "\n",
    "print(f\"SQL execution time: {sql_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance Comparison:\")\n",
    "print(f\"RDD approach:        {rdd_time:.4f} seconds\")\n",
    "print(f\"DataFrame approach:  {df_time:.4f} seconds\")\n",
    "print(f\"SQL approach:        {sql_time:.4f} seconds\")\n",
    "\n",
    "# Show execution plans\n",
    "print(\"\\n=== DataFrame Execution Plan ===\")\n",
    "df_result.explain()\n",
    "\n",
    "print(\"\\n=== SQL Execution Plan ===\")\n",
    "sql_result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify results are the same\n",
    "rdd_words = dict(rdd_result.collect())\n",
    "df_words = {row['word']: row['count'] for row in df_result.collect()}\n",
    "sql_words = {row['word']: row['count'] for row in sql_result.collect()}\n",
    "\n",
    "print(f\"Total unique words (RDD): {len(rdd_words)}\")\n",
    "print(f\"Total unique words (DataFrame): {len(df_words)}\")\n",
    "print(f\"Total unique words (SQL): {len(sql_words)}\")\n",
    "\n",
    "# Check if results match\n",
    "results_match = rdd_words == df_words == sql_words\n",
    "print(f\"\\nAll results match: {results_match}\")\n",
    "\n",
    "if not results_match:\n",
    "    print(\"Differences detected - checking common words...\")\n",
    "    common_words = set(rdd_words.keys()) & set(df_words.keys()) & set(sql_words.keys())\n",
    "    print(f\"Common words: {len(common_words)}\")\n",
    "\n",
    "print(\"\\n=== Key Insights ===\")\n",
    "print(\"1. RDD: Lower-level, more control, functional programming style\")\n",
    "print(\"2. DataFrame: Higher-level API, Catalyst optimizer, better performance\")\n",
    "print(\"3. SQL: Most readable, familiar syntax, same optimization as DataFrame\")\n",
    "print(\"4. For large datasets, DataFrame/SQL typically outperform RDD due to optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Reading from file (uncomment if you have input files)\n",
    "# file_rdd = spark.sparkContext.textFile(\"hdfs://path/to/input.txt\")\n",
    "# file_df = spark.read.text(\"hdfs://path/to/input.txt\")\n",
    "\n",
    "# Save results to different formats\n",
    "print(\"Saving results to different formats...\")\n",
    "\n",
    "# Save DataFrame as Parquet\n",
    "df_result.write.mode(\"overwrite\").parquet(\"/tmp/wordcount_parquet\")\n",
    "\n",
    "# Save DataFrame as JSON\n",
    "df_result.write.mode(\"overwrite\").json(\"/tmp/wordcount_json\")\n",
    "\n",
    "# Save RDD as text\n",
    "rdd_result.map(lambda x: f\"{x[0]}\\t{x[1]}\").saveAsTextFile(\"/tmp/wordcount_rdd\")\n",
    "\n",
    "print(\"Results saved to /tmp/wordcount_*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}